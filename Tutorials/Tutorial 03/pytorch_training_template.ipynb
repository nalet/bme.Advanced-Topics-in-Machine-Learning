{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61IOBJTdTzkN"
   },
   "source": [
    "# Tutorial 03\n",
    "\n",
    "## Cats vs Dogs\n",
    "From Tutorial 01. Dataset can be downloaded from http://files.fast.ai/data/dogscats.zip  \n",
    "**Classification task**, two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLAB VERSION**: https://colab.research.google.com/drive/1eWA7jI4R7JMZFLzRMAXI5ckCKZyBKSRQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_WG00x5T5VX"
   },
   "outputs": [],
   "source": [
    "# Run this only if you don't have the data already\n",
    "!wget http://files.fast.ai/data/dogscats.zip\n",
    "!unzip dogscats.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FNDlGFeLUCfO"
   },
   "outputs": [],
   "source": [
    "!pip install -U tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kaqhsFy0TzkP"
   },
   "source": [
    "# Training neural networks\n",
    "We have a set of input vectors $x$ (features) and desired targets (value / class label) $y$  \n",
    "We want to find a function $f$ such that $f(x)=y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AzFnCvOqTzkQ"
   },
   "source": [
    "Basic training algorithm:  \n",
    "**Input:**\n",
    "- features $x$, targets $y$  \n",
    "- neural network model with randomly initialized parameters $\\theta$: $f(x; \\theta)$  \n",
    "- learning_rate $\\alpha$  \n",
    "- loss function $L$  \n",
    "\n",
    "**Output:**\n",
    "- optimized parameters $\\theta$\n",
    "\n",
    "**Algorithm**  \n",
    ">for epoch in 1..n_epochs do:\n",
    ">>     for (minibatch_x, minibatch_y) in dataset do\n",
    ">>>        compute predicted y' = f(x; theta)\n",
    ">>>        compute loss = L(y', minibatch_y)\n",
    ">>>        compute gradients = gradient(L, theta)\n",
    ">>>        update parameters theta <- theta - alpha * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lt8EfFdDTzkR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g1g031EvTzkV"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, Compose\n",
    "root_dir = 'dogscats/train'\n",
    "\n",
    "target_size = (32, 32)\n",
    "transforms = Compose([Resize(target_size), # Resizes image\n",
    "                    ToTensor(),           # Converts to Tensor, scales to [0, 1] float (from [0, 255] int)\n",
    "                    Normalize(mean=(0.5, 0.5, 0.5,), std=(0.5, 0.5, 0.5)), # scales to [-1.0, 1.0]\n",
    "                    ])\n",
    "\n",
    "train_dataset_ = ImageFolder(root_dir, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb072mGeTzkY"
   },
   "outputs": [],
   "source": [
    "len(train_dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGqcfhXuTzkc"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow((train_dataset_[101][0]*0.5+0.5).numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OB7nvJTHTzkf"
   },
   "outputs": [],
   "source": [
    "class RAMDatasetWrapper(torch.utils.data.Dataset):\n",
    "    '''Loads all data into memory; good for small models and small datasets.'''\n",
    "    def __init__(self, dataset):\n",
    "        data = []\n",
    "        for sample in tqdm(dataset):\n",
    "            data.append(sample)\n",
    "        self.n = len(dataset)\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, ind):\n",
    "        return self.data[ind]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "train_dataset = RAMDatasetWrapper(train_dataset_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwCNc_srTzki"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4) #num_workers = n - how many threads in background for efficient loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbaFEYFpTzkl"
   },
   "outputs": [],
   "source": [
    "# Same for validation dataset\n",
    "val_root_dir = 'dogscats/valid'\n",
    "val_dataset_ = ImageFolder(val_root_dir, transform=transforms)\n",
    "val_dataset = RAMDatasetWrapper(val_dataset_)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlpQWsp3Tzkn"
   },
   "outputs": [],
   "source": [
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3_-_FshTzkq"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class LinearModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, 2, bias=True) # outputs 2 values - score for cat and score for dog\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = input.view(input.size(0), -1) # convert batch_size x 3 x imH x imW to batch_size x (3*imH*imW)\n",
    "        out = self.fc(out) # Applies out = input * A + b. A, b are parameters of nn.Linear that we want to learn\n",
    "        return out\n",
    "    \n",
    "class MLPModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLPModel, self).__init__()\n",
    "        pass # IMPLEMENT MLP MODEL\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JWOpV_lmTzku"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = LinearModel(32*32*3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize loss function, optimizer and parameters\n",
    "\n",
    "def train_epoch(model, train_dataloader, optimizer, loss_fn):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    # Iterate mini batches over training dataset\n",
    "        # Run predictions\n",
    "        # Set gradients to zero\n",
    "        # Compute loss\n",
    "        # Backpropagate (compute gradients)\n",
    "        # Make an optimization step (update parameters)\n",
    "        # Log metrics\n",
    "    # Return loss values for each iteration and accuracy\n",
    "    mean_loss = np.array(losses).mean()\n",
    "    return mean_loss, accuracy\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    # Iterate mini batches over validation dataset\n",
    "            # Run predictions\n",
    "            # Compute loss\n",
    "            # Save metrics\n",
    "    # Return mean loss and accuracy\n",
    "    mean_loss = np.array(losses).mean()\n",
    "    return mean_loss, accuracy\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, n_epochs, loss_function):\n",
    "    # We will monitor loss functions as the training progresses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # Run training for n_epochs\n",
    "        # train_losses.append(train_loss)\n",
    "        # val_losses.append(val_loss)\n",
    "        # train_accuracies.append(train_accuracy)\n",
    "        # val_accuracies.append(val_accuracy)\n",
    "        # print('Epoch {}/{}: train_loss: {:.4f}, train_accuracy: {:.4f}, val_loss: {:.4f}, val_accuracy: {:.4f}'.format(epoch+1, n_epochs,\n",
    "        #                                                                                               train_losses[-1],\n",
    "        #                                                                                               train_accuracies[-1],\n",
    "        #                                                                                               val_losses[-1],\n",
    "        #                                                                                               val_accuracies[-1]))\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2snR6NgzTzkw"
   },
   "outputs": [],
   "source": [
    "model = LinearModel(32*32*3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Train the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiGlFjX1Tzk1"
   },
   "outputs": [],
   "source": [
    "def plot(n_epochs, train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(n_epochs), train_losses)\n",
    "    plt.plot(np.arange(n_epochs), val_losses)\n",
    "    plt.legend(['train_loss', 'val_loss'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss value')\n",
    "    plt.title('Train/val loss');\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(n_epochs), train_accuracies)\n",
    "    plt.plot(np.arange(n_epochs), val_accuracies)\n",
    "    plt.legend(['train_acc', 'val_acc'])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('Train/val accuracy');\n",
    "\n",
    "plot(n_epochs, train_losses, val_losses, train_acc, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FxHuubBETzk3"
   },
   "outputs": [],
   "source": [
    "# Implement and train the MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dOhvxyd_WURF"
   },
   "outputs": [],
   "source": [
    "plot(n_epochs, train_losses, val_losses, train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NyIRZy24TzlB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JsLVI76vTzlD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorch-training-template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
