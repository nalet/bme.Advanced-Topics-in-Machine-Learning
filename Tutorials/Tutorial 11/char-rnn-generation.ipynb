{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 08 - Character Prediction with LSTMs\n",
    "\n",
    "This tutorial is based on the following two blog posts:\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://towardsdatascience.com/writing-like-shakespeare-with-machine-learning-in-pytorch-d77f851d910c\n",
    "\n",
    "\n",
    "The goal of this tutorial is to build a character level RNN and train it on a text corpus of some of Shakespeare's work. We will then use the trained RNN to hallucinate new text (one character at a time). \n",
    "\n",
    "![caption](charseq.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "We will use a small amount of Shakespears work to train the character prediction LSTM. Feel free to experiment with your own text data of choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the text file and show the first 100 characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open shakespeare text file and read in data as `text`\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Showing the first 100 characters\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We need to encode the characters in some way. We will simply map each unique character in the text to an integer (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 63, 31, 21, 23, 36, 14, 63, 23, 63, 62, 17, 49, 56, 47,  5, 17,\n",
       "       53,  1, 31, 17, 36, 24, 17, 36,  0, 31,  1, 46, 17, 17, 48, 36, 55,\n",
       "       49, 28, 36, 53, 38, 31, 23, 51, 17, 31, 19, 36, 51, 17, 55, 31, 36,\n",
       "       30, 17, 36, 21,  0, 17, 55, 26, 13, 47, 47, 27, 11, 11, 56, 47, 22,\n",
       "        0, 17, 55, 26, 19, 36, 21,  0, 17, 55, 26, 13, 47, 47, 10, 63, 31,\n",
       "       21, 23, 36, 14, 63, 23, 63, 62, 17, 49, 56, 47, 42,  1, 38])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding the text and map each character to an integer and vice versa\n",
    "\n",
    "# We create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# Encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "\n",
    "# Showing the first 100 encoded characters\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some helper functions that map the ineger IDs to one-hot labels. The one-hot encodings will be used as inputs and targets for the network training (similar to multi-class classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining method to encode one hot labels\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define a helper function that will create mini-batches for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining method to make mini-batches for training\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Recurrent Neural Network Model\n",
    "\n",
    "First check if a GPU is available for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it’s time to define the RNN network. We’re going to implement dropout for regularization and also create character dictionaries within the network. We’ll have LSTM units followed by a fully connected layer.\n",
    "\n",
    "We’ll name it Char-RNN because rather than having the input sequence be words, we’re going to look at the individual letters/characters instead.\n",
    "\n",
    "In the forward function, we’ll propagate the input and hidden state values through the LSTM layer to get the output and next hidden state values. After performing dropout, we reshape the output value to make it the proper dimensions for the fully connected layer.\n",
    "\n",
    "Finally, we also have a section in the RNN for initializing the hidden state values for the correct batch size if using mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the model\n",
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        #define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        #define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        #define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        #get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        #pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        #put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "\n",
    "Time for training! We declare a function, where we define an optimizer (Adam) and loss (cross entropy). We then create the training and validation data and initialize the hidden state of the RNN. \n",
    "We loop over the training set, each time encoding the data into one-hot vectors, performing forward and backpropagation, and updating the network parameters.\n",
    "\n",
    "Every once a while, we generate some loss statistics (training loss and validation loss) to let us know if the model is training correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the train method\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just declare the hyperparameters for our model, create an instance for it, and train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(65, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=65, bias=True)\n",
      ")\n",
      "Epoch: 1/20... Step: 50... Loss: 3.3330... Val Loss: 3.3380\n",
      "Epoch: 2/20... Step: 100... Loss: 3.2694... Val Loss: 3.2729\n",
      "Epoch: 2/20... Step: 150... Loss: 2.9064... Val Loss: 2.8759\n",
      "Epoch: 3/20... Step: 200... Loss: 2.5984... Val Loss: 2.5299\n",
      "Epoch: 4/20... Step: 250... Loss: 2.4060... Val Loss: 2.3536\n",
      "Epoch: 4/20... Step: 300... Loss: 2.2965... Val Loss: 2.2341\n",
      "Epoch: 5/20... Step: 350... Loss: 2.1986... Val Loss: 2.1361\n",
      "Epoch: 6/20... Step: 400... Loss: 2.1243... Val Loss: 2.0637\n",
      "Epoch: 6/20... Step: 450... Loss: 2.0567... Val Loss: 2.0118\n",
      "Epoch: 7/20... Step: 500... Loss: 1.9926... Val Loss: 1.9688\n",
      "Epoch: 8/20... Step: 550... Loss: 1.9433... Val Loss: 1.9222\n",
      "Epoch: 8/20... Step: 600... Loss: 1.8732... Val Loss: 1.8865\n",
      "Epoch: 9/20... Step: 650... Loss: 1.8484... Val Loss: 1.8548\n",
      "Epoch: 9/20... Step: 700... Loss: 1.7971... Val Loss: 1.8340\n",
      "Epoch: 10/20... Step: 750... Loss: 1.7736... Val Loss: 1.8070\n",
      "Epoch: 11/20... Step: 800... Loss: 1.7472... Val Loss: 1.7783\n",
      "Epoch: 11/20... Step: 850... Loss: 1.7212... Val Loss: 1.7587\n",
      "Epoch: 12/20... Step: 900... Loss: 1.7024... Val Loss: 1.7379\n",
      "Epoch: 13/20... Step: 950... Loss: 1.6914... Val Loss: 1.7155\n",
      "Epoch: 13/20... Step: 1000... Loss: 1.6517... Val Loss: 1.7068\n",
      "Epoch: 14/20... Step: 1050... Loss: 1.6330... Val Loss: 1.6900\n",
      "Epoch: 15/20... Step: 1100... Loss: 1.6516... Val Loss: 1.6689\n",
      "Epoch: 15/20... Step: 1150... Loss: 1.6065... Val Loss: 1.6611\n",
      "Epoch: 16/20... Step: 1200... Loss: 1.5389... Val Loss: 1.6484\n",
      "Epoch: 17/20... Step: 1250... Loss: 1.5908... Val Loss: 1.6317\n",
      "Epoch: 17/20... Step: 1300... Loss: 1.5782... Val Loss: 1.6241\n",
      "Epoch: 18/20... Step: 1350... Loss: 1.5330... Val Loss: 1.6137\n",
      "Epoch: 18/20... Step: 1400... Loss: 1.5465... Val Loss: 1.6004\n",
      "Epoch: 19/20... Step: 1450... Loss: 1.5359... Val Loss: 1.5971\n",
      "Epoch: 20/20... Step: 1500... Loss: 1.5171... Val Loss: 1.5880\n",
      "Epoch: 20/20... Step: 1550... Loss: 1.5070... Val Loss: 1.5773\n"
     ]
    }
   ],
   "source": [
    "# Define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)\n",
    "\n",
    "# Declaring the hyperparameters\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating new Text\n",
    "\n",
    "After training, we create a method to predict the next character from the trained RNN with forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a method to generate the next character\n",
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a sampling method that will use the previous method to generate an entire string of text, first using the characters in the first word and then using a loop to generate the next words using the top_k function, which chooses the letter with the highest probability to be next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring a method to generate new text\n",
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, just call the method, define the size you want (e.g., 1000 characters) and the starting character (e.g., ‘A’) and get the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ad, what, the person, thou hast true his fall,\n",
      "When he desires the persuned take and seads\n",
      "The mightess still with her to have be a serpent;\n",
      "These the most charge and himself, that I well be not\n",
      "Wompone well sight, trumpet talk to her their son\n",
      "The bods is that and men that stone the consures it\n",
      "Than have to bear treaton.\n",
      "\n",
      "CLARENCE:\n",
      "Thou day'st the since weeping to be a crief\n",
      "And will not teeth.'\n",
      "The steads some state, thou a stale storn with the\n",
      "send, there thither to his seeth was not all at time,\n",
      "And will see this too such a man; and steal a too\n",
      "to be he with a subblath of the struke of the\n",
      "forselves, and thy clooks and such, which was not bard,\n",
      "The way, a man of this some senter should have dead.\n",
      "\n",
      "KING EDWARD IV:\n",
      "Take a child bring of myself, thy doob.\n",
      "\n",
      "Second Minderer:\n",
      "And I will be make a sous of your highness\n",
      "Is so meaning, by my foult of you.\n",
      "\n",
      "Second Comen:\n",
      "And strike, thy brother with him.\n",
      "\n",
      "SAMPLOR:\n",
      "Why, so? thou wellst as all, and when you will be\n",
      "That is the cried.\n",
      "\n",
      "SICINIUS:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating new text\n",
    "print(sample(net, 1000, prime='A', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
